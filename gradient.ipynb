{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=0.1>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=-0.5>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算梯度\n",
    "x = tf.constant(-0.5)\n",
    "w = tf.constant(0.1)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch([w,x])\n",
    "    y = x*w\n",
    "tape.gradient(y,x),tape.gradient(y,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.23500371>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sigmoid 函数求梯度\n",
    "with tf.GradientTape() as tade:\n",
    "    tade.watch([x])\n",
    "    y=tf.sigmoid(x)\n",
    "tade.gradient(y,[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.7864477>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tanh 函数求梯度\n",
    "with tf.GradientTape() as tade:\n",
    "    tade.watch([x])\n",
    "    y = tf.tanh(x)\n",
    "tade.gradient(y,x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-0.1, shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=0.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.2>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu 函数求梯度\n",
    "with tf.GradientTape(persistent=True) as tade:\n",
    "    tade.watch([x])\n",
    "    y = tf.nn.relu(x)\n",
    "    g = tf.nn.leaky_relu(x)\n",
    "    print(g)\n",
    "tade.gradient(y,x) ,tade.gradient(g,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.17984962 0.17984962 0.19876459 0.19876459 0.24277161], shape=(5,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分类问题：softmax 求梯度\n",
    "a = tf.constant([0.1,0.1,0.2,0.2,0.4])\n",
    "with tf.GradientTape() as tade:\n",
    "    tade.watch([a])\n",
    "    y = tf.nn.softmax(a)\n",
    "    print(y)\n",
    "tade.gradient(y,a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(10, 3), dtype=float32, numpy=\n",
       " array([[ 3.29934191e-10,  1.38757430e-04, -1.38757750e-04],\n",
       "        [ 3.65298541e-10,  1.53630288e-04, -1.53630652e-04],\n",
       "        [ 8.23584922e-10,  3.46367655e-04, -3.46368470e-04],\n",
       "        [ 4.34392161e-10,  1.82688382e-04, -1.82688818e-04],\n",
       "        [ 5.08518505e-10,  2.13863023e-04, -2.13863532e-04],\n",
       "        [-1.11797238e-10, -4.70175510e-05,  4.70176637e-05],\n",
       "        [ 2.87282170e-10,  1.20819655e-04, -1.20819939e-04],\n",
       "        [ 2.47059994e-11,  1.03903785e-05, -1.03904031e-05],\n",
       "        [ 4.09973756e-11,  1.72418950e-05, -1.72419350e-05],\n",
       "        [ 1.43319689e-10,  6.02746586e-05, -6.02748041e-05]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 2.5620147e-11,  1.0774833e-05, -1.0774858e-05], dtype=float32)>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 交叉熵求梯度\n",
    "x = tf.random.normal([2,10],mean=10.,stddev=12.)\n",
    "w = tf.random.normal([10,3])\n",
    "b = tf.zeros([3])\n",
    "y = tf.constant([2,1])\n",
    "with tf.GradientTape() as tade:\n",
    "    tade.watch([w,b])\n",
    "    h = x@w+b\n",
    "    out = tf.nn.softmax(h)\n",
    "    out = tf.reduce_mean(tf.losses.categorical_crossentropy(tf.one_hot(y,3),out))\n",
    "tade.gradient(out,[w,b])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
